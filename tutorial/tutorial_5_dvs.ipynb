{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO HACK deleteme\n",
    "import sys\n",
    "import os\n",
    "PROJECT_DIR = \"/home/usr1/jboyle/neuro/sana-fe\"\n",
    "sys.path.insert(0, os.path.join(PROJECT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0330cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --extra-index-url https://test.pypi.org/simple/ sanafe==0.0.3\n",
    "%pip install pyyaml\n",
    "!wget -nc https://raw.githubusercontent.com/SLAM-Lab/SANA-FE/cpp/arch/loihi.yaml\n",
    "!wget -nc https://raw.githubusercontent.com/SLAM-Lab/SANA-FE/cpp/tutorial/dvs_challenge.npz\n",
    "import sanafe\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = sanafe.load_arch(\"loihi.yaml\")\n",
    "network = sanafe.Network()\n",
    "\n",
    "# Load the convolutional kernel weights, thresholds and input biases from file.\n",
    "#  If using the Docker container, this file is included in the image.\n",
    "#  Otherwise, this file is also hosted on Google Drive and can be downloaded\n",
    "#  prior to running this script\n",
    "import numpy as np\n",
    "try:\n",
    "    snn = np.load(\"dvs_challenge.npz\")\n",
    "except FileNotFoundError as exc:\n",
    "    print(exc)\n",
    "    print(\"\"\"\n",
    "To run this challenge, you need to download the network kernel weights: dvs_challenge.npz, to the tutorial directory.\n",
    "These weights are hosted online on a shared Google Drive. To download the file with a in Linux run the command:\n",
    "\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WkbJZFasTe-v8vTYXrUaz_1e-p_xHMEj' -O tutorial/dvs_challenge.npz\n",
    "\n",
    "Or go directly to the drive at: https://drive.google.com/drive/folders/1GzjXAFouakm3b6GcFIHsw67H8t6l3BtY?usp=drive_link\n",
    "          \"\"\")\n",
    "    exit()\n",
    "\n",
    "# Convert the DVS gesture categorization model to SANA-FE's SNN format\n",
    "thresholds = snn[\"thresholds\"]\n",
    "layer0 = network.create_neuron_group(\"in\", 1024, {\"threshold\": thresholds[0]})\n",
    "layer1 = network.create_neuron_group(\"conv1\", 3600, {\"threshold\": thresholds[1]})\n",
    "layer2 = network.create_neuron_group(\"conv2\", 5408, {\"threshold\": thresholds[2]})\n",
    "layer3 = network.create_neuron_group(\"conv3\", 7744, {\"threshold\": thresholds[3]})\n",
    "layer4 = network.create_neuron_group(\"conv4\", 891, {\"threshold\": thresholds[4]})\n",
    "layer5 = network.create_neuron_group(\"dense1\", 11, {\"threshold\": thresholds[5]})\n",
    "\n",
    "# Now define connectivity\n",
    "print(snn[\"conv1\"].shape)\n",
    "print(snn[\"conv1\"].flatten())\n",
    "# TODO: is the shape matching up with what I expect - should be channels last?\n",
    "# I liked the old interface better, so maybe I should support another level\n",
    "#  which basically allows us to create layers i.e. creates the neurons and connections\n",
    "#  in one go. Can have a deep learning python file\n",
    "layer0.connect_neurons_conv2d(layer1, {\"w\": snn[\"conv1\"].flatten()}, 32, 32, 1, 3, 3, kernel_count=16, stride_width=2, stride_height=2)\n",
    "layer1.connect_neurons_conv2d(layer2, {\"w\": snn[\"conv2\"].flatten()}, 15, 15, 16, 3, 3, kernel_count=32)\n",
    "layer2.connect_neurons_conv2d(layer3, {\"w\": snn[\"conv3\"].flatten()}, 13, 13, 32, 3, 3, kernel_count=64)\n",
    "layer3.connect_neurons_conv2d(layer4, {\"w\": snn[\"conv4\"].flatten()}, 11, 11, 64, 3, 3, kernel_count=11)\n",
    "layer4.connect_neurons_dense(layer5, {\"w\": snn[\"dense1\"].flatten()})\n",
    "\n",
    "biases = snn[\"inputs\"]\n",
    "# Finally set up the inputs\n",
    "for n, b in zip(layer0.neurons, biases):\n",
    "    n.configure(model_parameters={\"bias\": b})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the SNN to Loihi cores. Each layer is represented by its own neuron group\n",
    "### CHANGE THESE LINES TO TEST DIFFERENT MAPPINGS ###\n",
    "#sim.map_neuron_group_to_cores(layer0, arch, 1)\n",
    "#sim.map_neuron_group_to_cores(layer1, arch, 4)\n",
    "#sim.map_neuron_group_to_cores(layer2, arch, 16)\n",
    "#sim.map_neuron_group_to_cores(layer3, arch, 16)\n",
    "#sim.map_neuron_group_to_cores(layer4, arch, 4)\n",
    "#sim.map_neuron_group_to_cores(layer5, arch, 1)\n",
    "### END OF LINES TO CHANGE ###\n",
    "\n",
    "# TODO: get something like the above code again that's simpler\n",
    "for n in layer0.neurons:\n",
    "    n.map_to_core(arch.tiles[0].cores[0])\n",
    "\n",
    "for i, n in enumerate(layer1.neurons): # across 4 cores\n",
    "    core = i % 4\n",
    "    tile = core // 4\n",
    "    core_offset = core % 4\n",
    "    n.map_to_core(arch.tiles[2 + tile].cores[core_offset])\n",
    "\n",
    "for i, n in enumerate(layer2.neurons): # across 16 cores\n",
    "    total_neurons = len(layer2.neurons)\n",
    "    core = i % 16\n",
    "    tile = core // 4\n",
    "    core_offset = core % 4\n",
    "    n.map_to_core(arch.tiles[4 + tile].cores[core_offset])\n",
    "\n",
    "for i, n in enumerate(layer3.neurons): # across 16 cores\n",
    "    core = i % 16\n",
    "    tile = core // 4\n",
    "    core_offset = core % 4\n",
    "    n.map_to_core(arch.tiles[8 + tile].cores[core_offset])\n",
    "\n",
    "for i, n in enumerate(layer4.neurons): # Across 4 cores\n",
    "    core = i % 4\n",
    "    tile = core // 4\n",
    "    core_offset = core % 4\n",
    "    n.map_to_core(arch.tiles[12 + tile].cores[core_offset])\n",
    "\n",
    "for n in layer5.neurons:\n",
    "    n.map_to_core(arch.tiles[16].cores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b90dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip = sanafe.SpikingChip(arch)\n",
    "chip.load(network)\n",
    "# Run the network you just generated on Loihi\n",
    "# Comment out this line if you want to stop the simulations running\n",
    "results = chip.sim(10)\n",
    "expected_firing_neurons = 367770\n",
    "print(results)\n",
    "if results[\"neurons_fired\"] != expected_firing_neurons:\n",
    "    print(f\"Error: The total number of neurons spiking was \"\n",
    "          f\"{results['neurons_fired']}, \"\n",
    "          f\"should be {expected_firing_neurons}\")\n",
    "    print(\"Somehow you may have changed the functional behavior of the SNN\")\n",
    "    raise RuntimeError\n",
    "\n",
    "energy_delay_product = results[\"energy\"] * results[\"sim_time\"]\n",
    "print(f\"Energy-Delay product: {energy_delay_product}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
